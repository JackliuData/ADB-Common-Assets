# Databricks notebook source
# MAGIC %md-sandbox
# MAGIC 
# MAGIC <div style="text-align: center; line-height: 0; padding-top: 9px;">
# MAGIC   <img src="https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png" alt="Databricks Learning" style="width: 600px">
# MAGIC </div>

# COMMAND ----------

# MAGIC %md
# MAGIC # é’é“œå±‚ åˆ° é“¶å±‚ - ETL into a Silver table
# MAGIC 
# MAGIC æˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œä¸€äº›è½¬æ¢ï¼Œå°†å…¶ä»é’é“œè¡¨ç§»åŠ¨åˆ°é“¶è¡¨ã€‚

# COMMAND ----------

# MAGIC %md
# MAGIC ## ç¬”è®°æœ¬ç›®æ ‡
# MAGIC 
# MAGIC åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­:
# MAGIC 
# MAGIC 1. ä½¿ç”¨å¯ç»„åˆå‡½æ•°é‡‡é›†åŸå§‹æ•°æ®
# MAGIC 1. ä½¿ç”¨å¯ç»„åˆçš„å‡½æ•°æ¥å†™å…¥é’é“œè¡¨
# MAGIC 1. å‘å±•é’é“œåˆ°é“¶çš„æ­¥éª¤
# MAGIC   - æå–åŸå§‹å­—ç¬¦ä¸²å¹¶å°†å…¶è½¬æ¢ä¸ºåˆ—
# MAGIC   - éš”ç¦»ä¸è‰¯æ•°æ®
# MAGIC   - å°†å¹²å‡€çš„æ•°æ®åŠ è½½åˆ°Silverè¡¨ä¸­
# MAGIC 1. æ›´æ–°Bronzeè¡¨ä¸­çš„è®°å½•çŠ¶æ€

# COMMAND ----------

# MAGIC %md
# MAGIC ## é…ç½®

# COMMAND ----------

# MAGIC %run ./includes/configuration

# COMMAND ----------

# MAGIC %md
# MAGIC ## å¯¼å…¥æ“ä½œå‡½æ•°

# COMMAND ----------

# MAGIC %run ./includes/main/python/operations

# COMMAND ----------

# MAGIC %md
# MAGIC ### æ˜¾ç¤ºé’é“œè·¯å¾„ä¸­çš„æ–‡ä»¶

# COMMAND ----------

display(dbutils.fs.ls(bronzePath))

# COMMAND ----------

# MAGIC %md
# MAGIC ## è·å–æ›´å¤šåŸå§‹æ•°æ®
# MAGIC 
# MAGIC åœ¨æˆ‘ä»¬å¼€å§‹è¿™ä¸ªå®éªŒå®¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬è·å¾—æ›´å¤šçš„åŸå§‹æ•°æ®ã€‚
# MAGIC 
# MAGIC åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬å¯èƒ½æ¯å°æ—¶éƒ½æœ‰æ•°æ®ä¼ å…¥ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨å‡½æ•°ingest_classic_dataæ¥æ¨¡æ‹Ÿè¿™ä¸€ç‚¹ã€‚
# MAGIC 
# MAGIC ğŸ˜ å›æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬åœ¨ç¬”è®°æœ¬00_ingest_rawä¸­åšè¿‡è¿™ä¸ªæ“ä½œã€‚

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC **ç»ƒä¹ :**ä½¿ç”¨åŠŸèƒ½å‡½æ•°åŠ è½½10ä¸ªå°æ—¶ã€‚, `ingest_classic_data`.

# COMMAND ----------

# ANSWER
ingest_classic_data(hours=10)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Deltaå½“å‰æ¶æ„
# MAGIC æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åœ¨Deltaæ¶æ„ä¸­æ¼”ç¤ºåˆ°ç›®å‰ä¸ºæ­¢æ„å»ºçš„æ‰€æœ‰å†…å®¹ã€‚
# MAGIC 
# MAGIC æˆ‘ä»¬ä¸åƒä»¥å‰é‚£æ ·ä½¿ç”¨ä¸´æ—¶æŸ¥è¯¢æ¥å®ç°ï¼Œè€Œæ˜¯ä½¿ç”¨åŒ…å«åœ¨æ–‡ä»¶`classic/includes/main/python/operations`ä¸­çš„å¯ç»„åˆå‡½æ•°ã€‚ä½ åº”è¯¥æ£€æŸ¥è¿™ä¸ªæ–‡ä»¶ï¼Œä»¥ä¾¿åœ¨æ¥ä¸‹æ¥çš„ä¸‰æ­¥ä¸­ä½¿ç”¨æ­£ç¡®çš„å‚æ•°ã€‚
# MAGIC 
# MAGIC ğŸ¤” å¦‚æœä½ é‡åˆ°å›°éš¾ï¼Œå¯ä»¥å‚è€ƒ`02_bronze_to_silver`ã€‚

# COMMAND ----------

# MAGIC %md
# MAGIC ### ###ç¬¬ä¸€æ­¥:åˆ›å»º`rawDF`DataFrame
# MAGIC 
# MAGIC **ç»ƒä¹ :**ä½¿ç”¨`read_batch_raw`å‡½æ•°è·å–æ–°çš„æ•°æ®ã€‚

# COMMAND ----------

# ANSWER
rawDF = read_batch_raw(spark, rawPath)

# COMMAND ----------

# MAGIC %md
# MAGIC ### æ­¥éª¤2:è½¬æ¢åŸå§‹æ•°æ®
# MAGIC 
# MAGIC **ç»ƒä¹ :**ä½¿ç”¨`transform_raw`å‡½æ•°é‡‡é›†æ–°çš„æ•°æ®ã€‚

# COMMAND ----------

# ANSWER
transformedRawDF = transform_raw(rawDF)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ä½¿ç”¨æ–­è¨€éªŒè¯æ¨¡å¼
# MAGIC 
# MAGIC `transformedRawDF`dataframeç°åœ¨åº”è¯¥å…·æœ‰ä»¥ä¸‹æ¨¡å¼:
# MAGIC 
# MAGIC ```
# MAGIC datasource: string
# MAGIC ingesttime: timestamp
# MAGIC value: string
# MAGIC status: string
# MAGIC p_ingestdate: date
# MAGIC ```

# COMMAND ----------

from pyspark.sql.types import *

assert transformedRawDF.schema == StructType(
    [
        StructField("datasource", StringType(), False),
        StructField("ingesttime", TimestampType(), False),
        StructField("status", StringType(), False),
        StructField("value", StringType(), True),
        StructField("p_ingestdate", DateType(), False),
    ]
)
print("Assertion passed.")

# COMMAND ----------

# MAGIC %md
# MAGIC ### æ­¥éª¤2:æ‰¹å†™å…¥é’é“œè¡¨
# MAGIC 
# MAGIC **ç»ƒä¹ :** ä½¿ç”¨`batch_writer`å‡½æ•°æ¥é‡‡é›†æ–°çš„æ•°æ®ã€‚
# MAGIC 
# MAGIC **æ³¨æ„**:ä½ éœ€è¦åœ¨å†™å…¥å™¨ä¸Šä½¿ç”¨`.save()`æ–¹æ³•å¼€å§‹å†™å…¥ã€‚
# MAGIC 
# MAGIC ğŸ¤– **è¯·ç¡®ä¿åˆ†åŒºåœ¨`p_ingestdate`ä¸Š**.

# COMMAND ----------

# ANSWER
rawToBronzeWriter = batch_writer(dataframe=transformedRawDF, partition_column="p_ingestdate")

rawToBronzeWriter.save(bronzePath)

# COMMAND ----------

# MAGIC %md
# MAGIC ## æ¸…é™¤åŸå§‹æ–‡ä»¶è·¯å¾„
# MAGIC 
# MAGIC æ‰‹åŠ¨æ¸…é™¤å·²ç»åŠ è½½çš„åŸå§‹æ–‡ä»¶ã€‚

# COMMAND ----------

dbutils.fs.rm(rawPath, recurse=True)

# COMMAND ----------

# MAGIC %md
# MAGIC ## æ˜¾ç¤ºé’é“œè¡¨
# MAGIC 
# MAGIC å¦‚æœä½ æ‘„å…¥äº†16ä¸ªå°æ—¶ï¼Œä½ åº”è¯¥çœ‹åˆ°160æ¡è®°å½•ã€‚

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM health_tracker_classic_bronze

# COMMAND ----------

# MAGIC %md
# MAGIC ## é’é“œåˆ°é“¶
# MAGIC 
# MAGIC è®©æˆ‘ä»¬å¼€å§‹é’é“œåˆ°é“¶çš„æ­¥éª¤ã€‚

# COMMAND ----------

# MAGIC %md
# MAGIC ## ä½¿ç¬”è®°æœ¬ä¿æŒä¸€è‡´

# COMMAND ----------

dbutils.fs.rm(silverPath, recurse=True)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Load New Records from the Bronze Recordsä»é’é“œè®°å½•ä¸­åŠ è½½æ–°çš„è®°å½•
# MAGIC 
# MAGIC **ç»ƒä¹ **
# MAGIC 
# MAGIC ä»çŠ¶æ€ä¸º`"new"`çš„é’é“œè¡¨è¡¨ä¸­åŠ è½½æ‰€æœ‰è®°å½•ã€‚

# COMMAND ----------

# ANSWER

bronzeDF = spark.read.table("health_tracker_classic_bronze").filter("status = 'new'")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ä»é’é“œè®°å½•ä¸­æå–åµŒå¥—çš„JSON

# COMMAND ----------

# MAGIC %md
# MAGIC ### Step 1: ä»`value`åˆ—ä¸­æå–åµŒå¥—çš„JSON
# MAGIC **ç»ƒä¹ **
# MAGIC 
# MAGIC ä½¿ç”¨`pyspark.sql`å‡½æ•°å°†`value`åˆ—æå–ä¸ºæ–°åˆ—`nested_json`ã€‚

# COMMAND ----------

# ANSWER
from pyspark.sql.functions import from_json

json_schema = """
    time TIMESTAMP,
    name STRING,
    device_id STRING,
    steps INTEGER,
    day INTEGER,
    month INTEGER,
    hour INTEGER
"""

bronzeAugmentedDF = bronzeDF.withColumn("nested_json", from_json(col("value"), json_schema))

# COMMAND ----------

# MAGIC %md
# MAGIC ### Step 2: é€šè¿‡æ‹†è§£`nested_json`åˆ—æ¥åˆ›å»ºSilver DataFrame
# MAGIC 
# MAGIC è§£åŒ…JSONåˆ—æ„å‘³ç€æ‰å¹³åŒ–JSONï¼Œå°†æ¯ä¸ªé¡¶çº§å±æ€§ä½œä¸ºè‡ªå·±çš„åˆ—ã€‚
# MAGIC 
# MAGIC ğŸš¨ **é‡è¦å†…å®¹**è¯·ç¡®ä¿åœ¨Silver DataFrameä¸­åŒ…å«`"value"`åˆ—ï¼Œå› ä¸ºç¨åæˆ‘ä»¬å°†ä½¿ç”¨å®ƒä½œä¸ºé’é“œè¡¨ä¸­æ¯ä¸ªè®°å½•çš„å”¯ä¸€å¼•ç”¨

# COMMAND ----------

# ANSWER
silver_health_tracker = bronzeAugmentedDF.select("value", "nested_json.*")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ä½¿ç”¨æ–­è¨€éªŒè¯æ¨¡å¼
# MAGIC 
# MAGIC DataFrame `silver_health_tracker`ç°åœ¨åº”è¯¥å…·æœ‰ä»¥ä¸‹ç»“æ„:
# MAGIC 
# MAGIC ```
# MAGIC value: string
# MAGIC time: timestamp
# MAGIC name: string
# MAGIC device_id: string
# MAGIC steps: integer
# MAGIC day: integer
# MAGIC month: integer
# MAGIC hour: integer
# MAGIC ```
# MAGIC 
# MAGIC ğŸ’ªğŸ¼ è®°ä½ï¼Œ`_parse_datatype_string`å‡½æ•°å°†DDLæ ¼å¼çš„æ¨¡å¼å­—ç¬¦ä¸²è½¬æ¢ä¸ºSparkæ¨¡å¼ã€‚

# COMMAND ----------

from pyspark.sql.types import _parse_datatype_string

assert silver_health_tracker.schema == _parse_datatype_string(
    """
  value STRING,
  time TIMESTAMP,
  name STRING,
  device_id STRING,
  steps INTEGER,
  day INTEGER,
  month INTEGER,
  hour INTEGER
"""
)
print("Assertion passed.")

# COMMAND ----------

# MAGIC %md
# MAGIC ## è½¬æ¢æ•°æ®
# MAGIC 
# MAGIC 1. ä»åˆ—`time`ä¸­åˆ›å»ºä¸€åˆ—`p_eventdate DATE`ã€‚
# MAGIC 1. å°†åˆ—`time`é‡å‘½åä¸º`eventtime`ã€‚
# MAGIC 1. å°†`device_id`è½¬æ¢ä¸ºæ•´æ•°ã€‚
# MAGIC 1. æŒ‰æ­¤é¡ºåºåªåŒ…å«ä»¥ä¸‹åˆ—:
# MAGIC    1. `value`
# MAGIC    1. `device_id`
# MAGIC    1. `steps`
# MAGIC    1. `eventtime`
# MAGIC    1. `name`
# MAGIC    1. `p_eventdate`
# MAGIC 
# MAGIC ğŸ’ªğŸ¼ è¯·è®°ä½ï¼Œæˆ‘ä»¬å°†æ–°åˆ—å‘½åä¸º`p_eventdate`ï¼Œä»¥è¡¨æ˜æˆ‘ä»¬å¯¹è¿™ä¸€åˆ—è¿›è¡Œäº†åˆ†åŒºã€‚
# MAGIC 
# MAGIC ğŸ•µğŸ½â€â™€ï¸ è¯·è®°ä½ï¼Œæˆ‘ä»¬å°†`value`ä½œä¸ºé’é“œè¡¨ä¸­å€¼çš„å”¯ä¸€å¼•ç”¨ã€‚

# COMMAND ----------

# ANSWER
from pyspark.sql.functions import col

silver_health_tracker = silver_health_tracker.select(
    "value",
    col("device_id").cast("integer").alias("device_id"),
    "steps",
    col("time").alias("eventtime"),
    "name",
    col("time").cast("date").alias("p_eventdate"),
)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ä½¿ç”¨æ–­è¨€éªŒè¯æ¨¡å¼
# MAGIC 
# MAGIC DataFrame `silver_health_tracker_data_df`ç°åœ¨åº”è¯¥å…·æœ‰ä»¥ä¸‹æ¨¡å¼:
# MAGIC 
# MAGIC ```
# MAGIC value: string
# MAGIC device_id: integer
# MAGIC heartrate: double
# MAGIC eventtime: timestamp
# MAGIC name: string
# MAGIC p_eventdate: date```
# MAGIC 
# MAGIC ğŸ’ªğŸ¼ è®°ä½ï¼Œ`_parse_datatype_string`å‡½æ•°å°†DDLæ ¼å¼çš„æ¨¡å¼å­—ç¬¦ä¸²è½¬æ¢ä¸ºSparkæ¨¡å¼ã€‚

# COMMAND ----------

from pyspark.sql.types import _parse_datatype_string

assert silver_health_tracker.schema == _parse_datatype_string(
    """
  value STRING,
  device_id INTEGER,
  steps INTEGER,
  eventtime TIMESTAMP,
  name STRING,
  p_eventdate DATE
"""
), "Schemas do not match"
print("Assertion passed.")

# COMMAND ----------

# MAGIC %md
# MAGIC ## éš”ç¦»å¼‚å¸¸æ•°æ®
# MAGIC 
# MAGIC å›æƒ³ä¸€ä¸‹ï¼Œåœ¨`00_ingest_raw`æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å‘ç°ä¸€äº›è®°å½•ä»¥uuidå­—ç¬¦ä¸²è€Œä¸æ˜¯å­—ç¬¦ä¸²ç¼–ç çš„æ•´æ•°å½¢å¼ä¼ å…¥device_idã€‚æˆ‘ä»¬çš„é“¶è¡¨å°†device_idså­˜å‚¨ä¸ºæ•´æ•°ï¼Œå› æ­¤å¾ˆæ˜æ˜¾ä¼ å…¥çš„æ•°æ®å­˜åœ¨é—®é¢˜ã€‚
# MAGIC 
# MAGIC ä¸ºäº†æ­£ç¡®å¤„ç†æ­¤æ•°æ®è´¨é‡é—®é¢˜ï¼Œæˆ‘ä»¬å°†éš”ç¦»ä¸è‰¯è®°å½•ä»¥ä¾›åç»­å¤„ç†ã€‚

# COMMAND ----------

# MAGIC %md
# MAGIC æ£€æŸ¥æ˜¯å¦æœ‰nullè®°å½•â€”â€”æ¯”è¾ƒä¸‹é¢ä¸¤ä¸ªå•å…ƒæ ¼çš„è¾“å‡º

# COMMAND ----------

silver_health_tracker.count()

# COMMAND ----------

silver_health_tracker.na.drop().count()

# COMMAND ----------

# MAGIC %md
# MAGIC ### åˆ†å‰²é“¶çº§åˆ«DataFrame

# COMMAND ----------

silver_health_tracker_clean = silver_health_tracker.filter("device_id IS NOT NULL")
silver_health_tracker_quarantine = silver_health_tracker.filter("device_id IS NULL")

# COMMAND ----------

silver_health_tracker_quarantine.count()

# COMMAND ----------

# MAGIC %md
# MAGIC ### æ˜¾ç¤ºéš”ç¦»è®°å½•

# COMMAND ----------

display(silver_health_tracker_quarantine)

# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC ## å°†å¹²å‡€çš„æ•°æ®æ‰¹é‡å†™å…¥é“¶çº§åˆ«è¡¨
# MAGIC 
# MAGIC **ç»ƒä¹ :**æ‰¹é‡å†™å…¥`silver_health_tracker_clean`åˆ°é“¶è¡¨è·¯å¾„`silverPath`ä¸­ã€‚
# MAGIC 
# MAGIC 1. Use format, `"delta"`
# MAGIC 1. Use mode `"append"`.
# MAGIC 1. Do **NOT** include the `value` column.
# MAGIC 1. Partition by `"p_eventdate"`.

# COMMAND ----------

# ANSWER
(
    silver_health_tracker_clean.select("device_id", "steps", "eventtime", "name", "p_eventdate")
    .write.format("delta")
    .mode("append")
    .partitionBy("p_eventdate")
    .save(silverPath)
)

# COMMAND ----------

spark.sql(
    """
DROP TABLE IF EXISTS health_tracker_classic_silver
"""
)

spark.sql(
    f"""
CREATE TABLE health_tracker_classic_silver
USING DELTA
LOCATION "{silverPath}"
"""
)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ä½¿ç”¨æ–­è¨€éªŒè¯æ¨¡å¼

# COMMAND ----------

silverTable = spark.read.table("health_tracker_classic_silver")
expected_schema = """
  device_id INTEGER,
  steps INTEGER,
  eventtime TIMESTAMP,
  name STRING,
  p_eventdate DATE
"""

assert silverTable.schema == _parse_datatype_string(expected_schema), "Schemas do not match"
print("Assertion passed.")

# COMMAND ----------

# MAGIC %sql
# MAGIC 
# MAGIC SELECT * FROM health_tracker_classic_silver

# COMMAND ----------

# MAGIC %md
# MAGIC ## æ›´æ–°é’é“œè¡¨ä»¥ä½“ç°åŠ è½½
# MAGIC 
# MAGIC **ç»ƒä¹ :**æ›´æ–°é’é“œè¡¨ä¸­çš„è®°å½•ä»¥åæ˜ æ›´æ–°ã€‚
# MAGIC 
# MAGIC ### Step 1: æ›´æ–°å¹²å‡€çš„è®°å½•
# MAGIC å·²ç»åŠ è½½åˆ°é“¶çº§åˆ«è¡¨çš„å¹²å‡€è®°å½•ï¼Œå¹¶ä¸”åº”è¯¥å°†å…¶é’é“œè¡¨çš„`status`æ›´æ–°ä¸º`"loaded"`ã€‚
# MAGIC 
# MAGIC ğŸ’ƒğŸ½ **æç¤º**ä½ æ­£åœ¨å°†é“¶çº§åˆ«çš„dataframeä¸­çš„`value`åˆ—ä¸é’é“œè¡¨ä¸­çš„`value`åˆ—è¿›è¡ŒåŒ¹é…ã€‚

# COMMAND ----------

# ANSWER
from delta.tables import DeltaTable

bronzeTable = DeltaTable.forPath(spark, bronzePath)
silverAugmented = silver_health_tracker_clean.withColumn("status", lit("loaded"))

update_match = "bronze.value = clean.value"
update = {"status": "clean.status"}

(
    bronzeTable.alias("bronze")
    .merge(silverAugmented.alias("clean"), update_match)
    .whenMatchedUpdate(set=update)
    .execute()
)

# COMMAND ----------

# MAGIC %md
# MAGIC **ç»ƒä¹ :**æ›´æ–°é’é“œè¡¨ä¸­çš„è®°å½•ä»¥åæ˜ æ›´æ–°ã€‚
# MAGIC 
# MAGIC ### Step 2: æ›´æ–°éš”ç¦»è®°å½•
# MAGIC éš”ç¦»è®°å½•çš„é’é“œè¡¨â€œstatusâ€åº”æ›´æ–°ä¸ºâ€œquarantinedâ€ã€‚
# MAGIC 
# MAGIC ğŸ•ºğŸ» **æç¤º**ä½ æ­£åœ¨å°†éš”ç¦»çš„é“¶çº§åˆ«dataframeä¸­çš„`value`åˆ—ä¸é’é“œè¡¨ä¸­çš„`value`åˆ—è¿›è¡ŒåŒ¹é…ã€‚

# COMMAND ----------

# ANSWER
silverAugmented = silver_health_tracker_quarantine.withColumn("status", lit("quarantined"))

update_match = "bronze.value = quarantine.value"
update = {"status": "quarantine.status"}

(
    bronzeTable.alias("bronze")
    .merge(silverAugmented.alias("quarantine"), update_match)
    .whenMatchedUpdate(set=update)
    .execute()
)


# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC &copy; 2021 Databricks, Inc. All rights reserved.<br/>
# MAGIC Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a>.<br/>
# MAGIC <br/>
# MAGIC <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a> | <a href="http://help.databricks.com/">Support</a>

# COMMAND ----------

